
# ğŸ“˜ Claude/Gemini Clone with Mistral, LangChain, and FAISS

A full-stack conversational AI app that mimics the capabilities of tools like **Claude**, **Gemini**, or **ChatGPT** â€“ built entirely using open-source LLMs, LangChain, FAISS vector search, and Streamlit. This project answers questions about uploaded documents using semantic search and a conversational memory interface.

---

## ğŸš€ Demo

ğŸ”— [Live App on Streamlit]( https://clonegptgeminiclaudedeepseek-3d3t2mr9rvvcfdbp7ljspj.streamlit.app/)

---

## ğŸ“‚ Features

- âœ… Load any **PDF or TXT** document  
- âœ… Chunk and embed the content with **Sentence Transformers**  
- âœ… Create a searchable **FAISS vector store**  
- âœ… Use **Mistral-7B-Instruct** for high-quality answers  
- âœ… Maintain **chat history** using LangChain Memory  
- âœ… Clean and responsive **chat interface (like Claude or Gemini)**  
- âœ… Fully deployable on **Streamlit Cloud**  

---

## ğŸ› ï¸ Tech Stack

| Component        | Technology                            |
|------------------|----------------------------------------|
| LLM              | Mistral-7B-Instruct (Hugging Face)     |
| Vector Search    | FAISS                                  |
| Embeddings       | all-MiniLM-L6-v2 (Sentence Transformers)|
| Framework        | LangChain                              |
| UI               | Streamlit                              |
| Deployment       | Streamlit Cloud / Local                |

---

## ğŸ“ Folder Structure

```
ğŸ“ ClaudeGeminiClone/
â”œâ”€â”€ app.py                  # Streamlit frontend + backend
â”œâ”€â”€ faiss_index/            # FAISS vector store (generated after chunking)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Installation

### 1. Clone this repository

```bash
git clone https://github.com/your-username/ClaudeGeminiClone.git
cd ClaudeGeminiClone
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Run locally

```bash
streamlit run app.py
```

> âš ï¸ This project is heavy. It's recommended to use **Google Colab** or a **GPU-based cloud instance** for loading Mistral-7B.

---

## ğŸ”„ How It Works

1. User uploads a document (PDF or text)
2. Text is split into chunks
3. Chunks are embedded and indexed via FAISS
4. Mistral generates responses using LangChainâ€™s `ConversationalRetrievalChain`
5. Chat memory stores previous Q&A interactions

---

## ğŸ’¡ Use Cases

- Legal document summarization  
- Academic paper Q&A  
- Book or report assistant  
- Research assistant chatbot  
- Product documentation chatbot  

---

## ğŸ“¦ requirements.txt

```
transformers
torch
sentence-transformers
faiss-cpu
streamlit
langchain
```

---

## ğŸ‘¨â€ğŸ’» Author

> Built with passion and ambition to become a **top-level data scientist**.

**Mentored by:** GPT â€“ AI Research Assistant  
**Developer:** [Your Name](https://github.com/your-profile)

---

## ğŸ† Project 14 â€“ Done  
Next: **Project 15 â€“ Research-Level QA with LangGraph or RAG-as-Agent**
